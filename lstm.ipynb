{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device maintenance Task\n",
    "Given a data set containing data collected from sensors that are monitoring a  \n",
    "certain device a stakeholder perceived the need to improve the maintenance  \n",
    "of this device.  \n",
    "\n",
    "\n",
    "He approached you and described the Business case as:  \n",
    "We have this data and I understand that we need to improve the way we  \n",
    "schedule the maintenance for this device, can you assess this data and create  \n",
    "an AI solution for it?  \n",
    "\n",
    "\n",
    "The data is composed of 53 features and 1 Class.  \n",
    "One of the features is a timestamp, the others are sensor observations.  \n",
    "Can you help me to improve the maintenance somehow?  \n",
    "You should prepare the code and a short presentation (10 minutes max)  \n",
    "explaining your approach and why you decided to use the chosen approach.  \n",
    "\n",
    "### Deliverables:\n",
    "Code in an exported notebook  \n",
    "Presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q -U umap-learn\n",
    "# !pip install imbalanced-learn\n",
    "# ! pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, balanced_accuracy_score, confusion_matrix, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "import random\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import classification_report\n",
    "import utils\n",
    "pd.set_option('display.max_rows', 60)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(220320, 54)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>sensor_00</th>\n",
       "      <th>sensor_01</th>\n",
       "      <th>sensor_02</th>\n",
       "      <th>sensor_03</th>\n",
       "      <th>sensor_04</th>\n",
       "      <th>sensor_05</th>\n",
       "      <th>sensor_06</th>\n",
       "      <th>sensor_07</th>\n",
       "      <th>sensor_08</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor_43</th>\n",
       "      <th>sensor_44</th>\n",
       "      <th>sensor_45</th>\n",
       "      <th>sensor_46</th>\n",
       "      <th>sensor_47</th>\n",
       "      <th>sensor_48</th>\n",
       "      <th>sensor_49</th>\n",
       "      <th>sensor_50</th>\n",
       "      <th>sensor_51</th>\n",
       "      <th>machine_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-04-01 00:00:00</td>\n",
       "      <td>2.465394</td>\n",
       "      <td>47.09201</td>\n",
       "      <td>53.2118</td>\n",
       "      <td>46.310760</td>\n",
       "      <td>634.3750</td>\n",
       "      <td>76.45975</td>\n",
       "      <td>13.41146</td>\n",
       "      <td>16.13136</td>\n",
       "      <td>15.56713</td>\n",
       "      <td>...</td>\n",
       "      <td>41.92708</td>\n",
       "      <td>39.641200</td>\n",
       "      <td>65.68287</td>\n",
       "      <td>50.92593</td>\n",
       "      <td>38.194440</td>\n",
       "      <td>157.9861</td>\n",
       "      <td>67.70834</td>\n",
       "      <td>243.0556</td>\n",
       "      <td>201.3889</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-04-01 00:01:00</td>\n",
       "      <td>2.465394</td>\n",
       "      <td>47.09201</td>\n",
       "      <td>53.2118</td>\n",
       "      <td>46.310760</td>\n",
       "      <td>634.3750</td>\n",
       "      <td>76.45975</td>\n",
       "      <td>13.41146</td>\n",
       "      <td>16.13136</td>\n",
       "      <td>15.56713</td>\n",
       "      <td>...</td>\n",
       "      <td>41.92708</td>\n",
       "      <td>39.641200</td>\n",
       "      <td>65.68287</td>\n",
       "      <td>50.92593</td>\n",
       "      <td>38.194440</td>\n",
       "      <td>157.9861</td>\n",
       "      <td>67.70834</td>\n",
       "      <td>243.0556</td>\n",
       "      <td>201.3889</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-04-01 00:02:00</td>\n",
       "      <td>2.444734</td>\n",
       "      <td>47.35243</td>\n",
       "      <td>53.2118</td>\n",
       "      <td>46.397570</td>\n",
       "      <td>638.8889</td>\n",
       "      <td>73.54598</td>\n",
       "      <td>13.32465</td>\n",
       "      <td>16.03733</td>\n",
       "      <td>15.61777</td>\n",
       "      <td>...</td>\n",
       "      <td>41.66666</td>\n",
       "      <td>39.351852</td>\n",
       "      <td>65.39352</td>\n",
       "      <td>51.21528</td>\n",
       "      <td>38.194443</td>\n",
       "      <td>155.9606</td>\n",
       "      <td>67.12963</td>\n",
       "      <td>241.3194</td>\n",
       "      <td>203.7037</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-04-01 00:03:00</td>\n",
       "      <td>2.460474</td>\n",
       "      <td>47.09201</td>\n",
       "      <td>53.1684</td>\n",
       "      <td>46.397568</td>\n",
       "      <td>628.1250</td>\n",
       "      <td>76.98898</td>\n",
       "      <td>13.31742</td>\n",
       "      <td>16.24711</td>\n",
       "      <td>15.69734</td>\n",
       "      <td>...</td>\n",
       "      <td>40.88541</td>\n",
       "      <td>39.062500</td>\n",
       "      <td>64.81481</td>\n",
       "      <td>51.21528</td>\n",
       "      <td>38.194440</td>\n",
       "      <td>155.9606</td>\n",
       "      <td>66.84028</td>\n",
       "      <td>240.4514</td>\n",
       "      <td>203.1250</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-04-01 00:04:00</td>\n",
       "      <td>2.445718</td>\n",
       "      <td>47.13541</td>\n",
       "      <td>53.2118</td>\n",
       "      <td>46.397568</td>\n",
       "      <td>636.4583</td>\n",
       "      <td>76.58897</td>\n",
       "      <td>13.35359</td>\n",
       "      <td>16.21094</td>\n",
       "      <td>15.69734</td>\n",
       "      <td>...</td>\n",
       "      <td>41.40625</td>\n",
       "      <td>38.773150</td>\n",
       "      <td>65.10416</td>\n",
       "      <td>51.79398</td>\n",
       "      <td>38.773150</td>\n",
       "      <td>158.2755</td>\n",
       "      <td>66.55093</td>\n",
       "      <td>242.1875</td>\n",
       "      <td>201.3889</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp  sensor_00  sensor_01  sensor_02  sensor_03  sensor_04  \\\n",
       "0  2018-04-01 00:00:00   2.465394   47.09201    53.2118  46.310760   634.3750   \n",
       "1  2018-04-01 00:01:00   2.465394   47.09201    53.2118  46.310760   634.3750   \n",
       "2  2018-04-01 00:02:00   2.444734   47.35243    53.2118  46.397570   638.8889   \n",
       "3  2018-04-01 00:03:00   2.460474   47.09201    53.1684  46.397568   628.1250   \n",
       "4  2018-04-01 00:04:00   2.445718   47.13541    53.2118  46.397568   636.4583   \n",
       "\n",
       "   sensor_05  sensor_06  sensor_07  sensor_08  ...  sensor_43  sensor_44  \\\n",
       "0   76.45975   13.41146   16.13136   15.56713  ...   41.92708  39.641200   \n",
       "1   76.45975   13.41146   16.13136   15.56713  ...   41.92708  39.641200   \n",
       "2   73.54598   13.32465   16.03733   15.61777  ...   41.66666  39.351852   \n",
       "3   76.98898   13.31742   16.24711   15.69734  ...   40.88541  39.062500   \n",
       "4   76.58897   13.35359   16.21094   15.69734  ...   41.40625  38.773150   \n",
       "\n",
       "   sensor_45  sensor_46  sensor_47  sensor_48  sensor_49  sensor_50  \\\n",
       "0   65.68287   50.92593  38.194440   157.9861   67.70834   243.0556   \n",
       "1   65.68287   50.92593  38.194440   157.9861   67.70834   243.0556   \n",
       "2   65.39352   51.21528  38.194443   155.9606   67.12963   241.3194   \n",
       "3   64.81481   51.21528  38.194440   155.9606   66.84028   240.4514   \n",
       "4   65.10416   51.79398  38.773150   158.2755   66.55093   242.1875   \n",
       "\n",
       "   sensor_51  machine_status  \n",
       "0   201.3889          NORMAL  \n",
       "1   201.3889          NORMAL  \n",
       "2   203.7037          NORMAL  \n",
       "3   203.1250          NORMAL  \n",
       "4   201.3889          NORMAL  \n",
       "\n",
       "[5 rows x 54 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/DATA.csv', index_col=0)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sensor_15         220320\n",
       "sensor_50          77017\n",
       "sensor_51          15383\n",
       "sensor_00          10208\n",
       "sensor_07           5451\n",
       "sensor_08           5107\n",
       "sensor_06           4798\n",
       "sensor_09           4595\n",
       "sensor_01            369\n",
       "sensor_30            261\n",
       "sensor_29             72\n",
       "sensor_32             68\n",
       "sensor_18             46\n",
       "sensor_17             46\n",
       "sensor_22             41\n",
       "sensor_25             36\n",
       "sensor_16             31\n",
       "sensor_49             27\n",
       "sensor_48             27\n",
       "sensor_47             27\n",
       "sensor_46             27\n",
       "sensor_45             27\n",
       "sensor_44             27\n",
       "sensor_43             27\n",
       "sensor_42             27\n",
       "sensor_41             27\n",
       "sensor_40             27\n",
       "sensor_39             27\n",
       "sensor_38             27\n",
       "sensor_14             21\n",
       "sensor_26             20\n",
       "sensor_03             19\n",
       "sensor_10             19\n",
       "sensor_13             19\n",
       "sensor_12             19\n",
       "sensor_11             19\n",
       "sensor_05             19\n",
       "sensor_04             19\n",
       "sensor_02             19\n",
       "sensor_36             16\n",
       "sensor_37             16\n",
       "sensor_28             16\n",
       "sensor_27             16\n",
       "sensor_31             16\n",
       "sensor_35             16\n",
       "sensor_24             16\n",
       "sensor_23             16\n",
       "sensor_34             16\n",
       "sensor_21             16\n",
       "sensor_20             16\n",
       "sensor_19             16\n",
       "sensor_33             16\n",
       "timestamp              0\n",
       "machine_status         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop empty column\n",
    "df.drop([\"sensor_15\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['timestamp'] = pd.to_datetime(df['timestamp'], format='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 Index(['timestamp', 'sensor_00', 'sensor_01', 'sensor_02', 'sensor_03',\n",
      "       'sensor_04', 'sensor_05', 'sensor_06', 'sensor_07', 'sensor_08',\n",
      "       'sensor_09', 'sensor_10', 'sensor_11', 'sensor_12', 'sensor_13',\n",
      "       'sensor_14', 'sensor_16', 'sensor_17', 'sensor_18', 'sensor_19',\n",
      "       'sensor_20', 'sensor_21', 'sensor_22', 'sensor_23', 'sensor_24',\n",
      "       'sensor_25', 'sensor_26', 'sensor_27', 'sensor_28', 'sensor_29',\n",
      "       'sensor_30', 'sensor_31', 'sensor_32', 'sensor_33', 'sensor_34',\n",
      "       'sensor_35', 'sensor_36', 'sensor_37', 'sensor_38', 'sensor_39',\n",
      "       'sensor_40', 'sensor_41', 'sensor_42', 'sensor_43', 'sensor_44',\n",
      "       'sensor_45', 'sensor_46', 'sensor_47', 'sensor_48', 'sensor_49',\n",
      "       'sensor_50', 'sensor_51'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "cols = df.columns[:-1]\n",
    "print(len(cols), cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, cols = utils.preprocess(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering\n",
    "Remove maintenance and last normal period\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(205843, 55)\n"
     ]
    }
   ],
   "source": [
    "df = df[df[\"machine_status\"] != \"MAINTENANCE\"]\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(152039, 55)\n"
     ]
    }
   ],
   "source": [
    "df = df[df['idx'] <= df[df[\"machine_status\"] == \"BROKEN\"][\"idx\"].values[-1]]\n",
    "print(df.shape)\n",
    "df['idx'] = np.arange(df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create survival columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev = 0\n",
    "survival = np.zeros(df.shape[0])\n",
    "incident = np.zeros(df.shape[0])\n",
    "broken_idx = df[df['y'] == 0]['idx'].values\n",
    "for ii, i in enumerate(broken_idx):\n",
    "    survival[prev:i+1] = np.arange(i-prev+1)[::-1]\n",
    "    incident[prev:i+1] = np.ones(i-prev+1) * ii\n",
    "    prev = i + 1\n",
    "df['survival'] = survival.astype(int)\n",
    "df['incident_nb'] = incident.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset creation for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index(np.arange(len(df)), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7599, 51, 20) (7599,) Counter({0: 7249, 1: 350})\n"
     ]
    }
   ],
   "source": [
    "prediction_horizon = 1000\n",
    "window_size = 20\n",
    "X, y_reg, y_classif, incident_ref, idx_ref = utils.prepare_dataset(df, cols, window_size,  prediction_horizon, autocorr_window=1)\n",
    "print(X.shape, y_reg.shape, Counter(y_classif))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 5074, 1: 245}) Counter({0: 2175, 1: 105})\n"
     ]
    }
   ],
   "source": [
    "idx_train, idx_test, Y_train, Y_test = train_test_split(np.arange(len(y_classif)), \n",
    "    y_classif, test_size=0.3, random_state=1, stratify=y_classif)\n",
    "print(Counter(Y_train), Counter(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.tensor(X[idx_train].transpose(0,2,1), dtype=torch.float32), torch.tensor(y_classif[idx_train], dtype=torch.long))\n",
    "test_dataset = TensorDataset(torch.tensor(X[idx_test].transpose(0,2,1), dtype=torch.float32), torch.tensor(y_classif[idx_test], dtype=torch.long))\n",
    "train_loader = DataLoader(train_dataset, batch_size=50, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=50, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMClassifier(\n",
      "  (lstm): LSTM(51, 100, num_layers=2, batch_first=True)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=1, bias=True)\n",
      "  )\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # LSTM with hidden states\n",
    "        lstm_out, (hidden, _) = self.lstm(x)\n",
    "        # Only take the output from the last time step\n",
    "        hidden = hidden[-1]\n",
    "        out = self.classifier(hidden)\n",
    "        return out\n",
    "\n",
    "# Parameters for the LSTM model\n",
    "input_dim = 51  # number of features per timestep\n",
    "hidden_dim = 100  # hidden layer size\n",
    "output_dim = 1  # number of classes\n",
    "num_layers = 2  # number of LSTM layers\n",
    "\n",
    "# Initialize the LSTM model\n",
    "model = LSTMClassifier(input_dim, hidden_dim, output_dim, num_layers)\n",
    "print(model)\n",
    "# model(xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [107/107], train loss 41.14000126346946\n",
      "Epoch [2/100], Step [107/107], train loss 13.437049891799688\n",
      "Epoch [3/100], Step [107/107], train loss 13.391793897375464\n",
      "Epoch [4/100], Step [107/107], train loss 13.371289612725377\n",
      "Epoch [5/100], Step [107/107], train loss 13.25111173838377\n",
      "Epoch [6/100], Step [107/107], train loss 12.946743007749319\n",
      "Epoch [7/100], Step [107/107], train loss 12.867713704705238\n",
      "Epoch [8/100], Step [107/107], train loss 12.61963115632534\n",
      "Epoch [9/100], Step [107/107], train loss 12.640232594683766\n",
      "Epoch [10/100], Step [107/107], train loss 12.759952707216144\n",
      "Epoch [11/100], Step [107/107], train loss 12.60412055812776\n",
      "Epoch [12/100], Step [107/107], train loss 12.610445758327842\n",
      "Epoch [13/100], Step [107/107], train loss 12.448856864124537\n",
      "Epoch [14/100], Step [107/107], train loss 12.271467560902238\n",
      "Epoch [15/100], Step [107/107], train loss 12.231596982106566\n",
      "Epoch [16/100], Step [107/107], train loss 12.349553745239973\n",
      "Epoch [17/100], Step [107/107], train loss 11.839208470657468\n",
      "Epoch [18/100], Step [107/107], train loss 11.134374363347888\n",
      "Epoch [19/100], Step [107/107], train loss 11.229460207745433\n",
      "Epoch [20/100], Step [107/107], train loss 11.003002539277077\n",
      "Epoch [21/100], Step [107/107], train loss 10.583456825464964\n",
      "Epoch [22/100], Step [107/107], train loss 10.16643018182367\n",
      "Epoch [23/100], Step [107/107], train loss 11.394924690946937\n",
      "Epoch [24/100], Step [107/107], train loss 10.860866443254054\n",
      "Epoch [25/100], Step [107/107], train loss 9.957088187336922\n",
      "Epoch [26/100], Step [107/107], train loss 9.856356287375093\n",
      "Epoch [27/100], Step [107/107], train loss 9.793519011698663\n",
      "Epoch [28/100], Step [107/107], train loss 9.36812587082386\n",
      "Epoch [29/100], Step [107/107], train loss 9.26900821737945\n",
      "Epoch [30/100], Step [107/107], train loss 8.664743818808347\n",
      "Epoch [31/100], Step [107/107], train loss 8.2017713310197\n",
      "Epoch [32/100], Step [107/107], train loss 8.303306804969907\n",
      "Epoch [33/100], Step [107/107], train loss 7.695761057548225\n",
      "Epoch [34/100], Step [107/107], train loss 7.487440147437155\n",
      "Epoch [35/100], Step [107/107], train loss 7.5957327974028885\n",
      "Epoch [36/100], Step [107/107], train loss 6.80099944351241\n",
      "Epoch [37/100], Step [107/107], train loss 6.915351880248636\n",
      "Epoch [38/100], Step [107/107], train loss 5.8721600354183465\n",
      "Epoch [39/100], Step [107/107], train loss 6.200158569961786\n",
      "Epoch [40/100], Step [107/107], train loss 6.3685470866039395\n",
      "Epoch [41/100], Step [107/107], train loss 5.822874351404607\n",
      "Epoch [42/100], Step [107/107], train loss 5.510564331896603\n",
      "Epoch [43/100], Step [107/107], train loss 5.599255244247615\n",
      "Epoch [44/100], Step [107/107], train loss 5.516273725312203\n",
      "Epoch [45/100], Step [107/107], train loss 5.540906292852014\n",
      "Epoch [46/100], Step [107/107], train loss 4.722423094790429\n",
      "Epoch [47/100], Step [107/107], train loss 4.7929365274030715\n",
      "Epoch [48/100], Step [107/107], train loss 4.70069486903958\n",
      "Epoch [49/100], Step [107/107], train loss 4.992512514698319\n",
      "Epoch [50/100], Step [107/107], train loss 4.449944345746189\n",
      "Epoch [51/100], Step [107/107], train loss 4.730802529491484\n",
      "Epoch [52/100], Step [107/107], train loss 4.136752746882848\n",
      "Epoch [53/100], Step [107/107], train loss 4.843516971450299\n",
      "Epoch [54/100], Step [107/107], train loss 4.38222383079119\n",
      "Epoch [55/100], Step [107/107], train loss 4.845632393320557\n",
      "Epoch [56/100], Step [107/107], train loss 4.2332200419623405\n",
      "Epoch [57/100], Step [107/107], train loss 3.8259001860860735\n",
      "Epoch [58/100], Step [107/107], train loss 3.675065225805156\n",
      "Epoch [59/100], Step [107/107], train loss 6.596515184122836\n",
      "Epoch [60/100], Step [107/107], train loss 4.437513498123735\n",
      "Epoch [61/100], Step [107/107], train loss 3.2729529194766656\n",
      "Epoch [62/100], Step [107/107], train loss 4.6419868767261505\n",
      "Epoch [63/100], Step [107/107], train loss 3.664194943383336\n",
      "Epoch [64/100], Step [107/107], train loss 3.2496746918186545\n",
      "Epoch [65/100], Step [107/107], train loss 4.0112668219953775\n",
      "Epoch [66/100], Step [107/107], train loss 3.209923035465181\n",
      "Epoch [67/100], Step [107/107], train loss 3.1745173478266224\n",
      "Epoch [68/100], Step [107/107], train loss 4.360674163443036\n",
      "Epoch [69/100], Step [107/107], train loss 3.076615875470452\n",
      "Epoch [70/100], Step [107/107], train loss 3.51340783177875\n",
      "Epoch [71/100], Step [107/107], train loss 3.0591254162136465\n",
      "Epoch [72/100], Step [107/107], train loss 3.4782048733904958\n",
      "Epoch [73/100], Step [107/107], train loss 4.448731308162678\n",
      "Epoch [74/100], Step [107/107], train loss 2.9938801841344684\n",
      "Epoch [75/100], Step [107/107], train loss 2.8833770926576108\n",
      "Epoch [76/100], Step [107/107], train loss 2.769496214692481\n",
      "Epoch [77/100], Step [107/107], train loss 2.653045889106579\n",
      "Epoch [78/100], Step [107/107], train loss 3.485207618912682\n",
      "Epoch [79/100], Step [107/107], train loss 2.6109850089997053\n",
      "Epoch [80/100], Step [107/107], train loss 2.8757581670652144\n",
      "Epoch [81/100], Step [107/107], train loss 5.23018105677329\n",
      "Epoch [82/100], Step [107/107], train loss 2.5076905669411644\n",
      "Epoch [83/100], Step [107/107], train loss 3.0672598889796063\n",
      "Epoch [84/100], Step [107/107], train loss 2.5587996606482193\n",
      "Epoch [85/100], Step [107/107], train loss 2.4316338414209895\n",
      "Epoch [86/100], Step [107/107], train loss 2.6110095400363207\n",
      "Epoch [87/100], Step [107/107], train loss 2.4408831977052614\n",
      "Epoch [88/100], Step [107/107], train loss 3.313782610843191\n",
      "Epoch [89/100], Step [107/107], train loss 2.4090397976106033\n",
      "Epoch [90/100], Step [107/107], train loss 3.8568499528919347\n",
      "Epoch [91/100], Step [107/107], train loss 2.667589837103151\n",
      "Epoch [92/100], Step [107/107], train loss 2.5906426262808964\n",
      "Epoch [93/100], Step [107/107], train loss 2.8347276760614477\n",
      "Epoch [94/100], Step [107/107], train loss 2.081231215968728\n",
      "Epoch [95/100], Step [107/107], train loss 3.0014063895796426\n",
      "Epoch [96/100], Step [107/107], train loss 2.745561432966497\n",
      "Epoch [97/100], Step [107/107], train loss 1.9526996925706044\n",
      "Epoch [98/100], Step [107/107], train loss 2.648839993722504\n",
      "Epoch [99/100], Step [107/107], train loss 2.086982562614139\n",
      "Epoch [100/100], Step [107/107], train loss 2.1345099229365587\n"
     ]
    }
   ],
   "source": [
    "class_weights = torch.tensor([0.6])\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "# Train the model\n",
    "num_epochs = 100\n",
    "total_loss = 0\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for i, (inputs, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, target.float().view(-1, 1))\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], train loss {total_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0.0: 2195, 1.0: 85}) Counter({0: 2175, 1: 105})\n",
      "0.8\n"
     ]
    }
   ],
   "source": [
    "predictions =[]\n",
    "truth = []\n",
    "for i, (inputs, target) in enumerate(test_loader):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    probabilities = torch.sigmoid(outputs)\n",
    "    predictions.append((probabilities.detach() > 0.5).float().numpy())\n",
    "    truth.append(target.numpy())\n",
    "\n",
    "predictions = np.concatenate(predictions).flatten()\n",
    "target = np.concatenate(truth)\n",
    "print(Counter(predictions), Counter(target))\n",
    "print(f1_score(target, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
